
resume_training: False

experiment:
  experiments_base_dir: /home/ubuntu/workspace/experiments
  project_name: RNAformer
  session_name: rnaformer_1
  experiment_name: ts0_conform_test-0002


trainer:
  num_nodes: 1
  check_val_every_n_epoch: null #1
  default_root_dir: /home/ubuntu/workspace/
  enable_checkpointing: true
  enable_model_summary: true
  enable_progress_bar: true
  accelerator: 'gpu'
  devices: 1
  gradient_clip_val: 0.1
  limit_train_batches: null
  limit_val_batches: null
  log_every_n_steps: 10
  max_epochs: null
  max_steps: 100000
  num_sanity_val_steps: 2
  precision: 16
  reload_dataloaders_every_n_epochs: 1
  replace_sampler_ddp: false
  resume_from_checkpoint: null
  track_grad_norm: -1
  val_check_interval: 100



train:
  seed: 1234
  neg_samples: False # 500  # calculates the softmax CE loss only over #neg_samples+1 words
  softmax_temp: False  # calculates the softmax with temperature

  optimizer:
    _target_: torch.optim.AdamW
    lr: 0.001
    weight_decay: 0.1
    betas:
      - 0.9
      - 0.98
    eps: 1.0e-09
    adam_w_mode: true
    seed: 1234
    scheduler_mult_factor: null
  optimizer_param_grouping:
    bias_weight_decay: False
    normalization_weight_decay: False
  scheduler:
    num_warmup_steps: 200 #${eval:0.01 * ${trainer.max_steps}}
    num_training_steps: ${trainer.max_steps}
    decay_factor: 0.1
    schedule: "cosine" # "cosine" or "linear"
  loss_fn:
    inplace_backward: True  # to save memory



callbacks:
  learning_rate_monitor:
    _target_: pytorch_lightning.callbacks.LearningRateMonitor
    logging_interval: step
  model_checkpoint:
    _target_: pytorch_lightning.callbacks.ModelCheckpoint
    dirpath: ${experiment.experiments_base_dir}
    auto_insert_metric_name: False
    every_n_train_steps: 10000
    every_n_epochs: null
    save_top_k: 1
    monitor: "step"
    mode: "max"
    filename: "checkpoint-{epoch:02d}-{global_step}"
    save_last: True


logger:
  tensorboard:
    _target_: pytorch_lightning.loggers.tensorboard.TensorBoardLogger
    save_dir: "tensorboard/"
    name: ""
    version: "tb"
    log_graph: False
    default_hp_metric: True
    prefix: ""


deepspeed:
  zero_optimization: True
  stage: 2
  contiguous_gradients: false
  allgather_bucket_size: 5e8
  reduce_bucket_size: 5e8
  overlap_comm: true
  zero_allow_untested_optimizer: true


RNAformer:
  precision: ${trainer.precision}
  seq_vocab_size: 0
  trg_vocab_size: 0
  max_len: 0

  model_dim: 256 # hidden dimension of transformer
  n_layers: 6  # number of transformer layers
  num_head: 4  # number of heads per layer

  ff_factor: 4  # hidden dim * ff_factor = size of feed-forward layer
  ff_kernel: 3

  cycling: 6

  resi_dropout: 0.1
  embed_dropout: 0.1

  rel_pos_enc: True  # relative position encoding
  head_bias: False
  ln_eps: 1e-5

  softmax_scale: True
  key_dim_scaler: True
  gating: False
  use_glu: False
  use_bias: true

  flash_attn: false

  initializer_range: 0.02
  zero_init: false  # init last layer per block before each residual connection

rna_data:
  dataframe_path: "/data/bprna_dataset.plk"
  valid_sets: ['bprna_vl0.plk', 'pdb_vl1']
  test_sets: ['pdb_ts1', 'pdb_ts2', 'pdb_ts3', 'pdb_ts_hard', 'pdb_puzzles24',]
  oversample_pdb: 1
  predict_canonical: false
  random_ignore_mat: 0.5
  partial_training: false
  design: False
  num_cpu_worker: 10
  num_gpu_worker: ${trainer.devices}
  min_len: 2
  max_len: 100
  similarity: 80
  seed: 1
  batch_size: 2
  batch_by_token_size: false
  batch_token_size: 400
  shuffle_pool_size: 20
  cache_dir: "/tmp/data/cache"
